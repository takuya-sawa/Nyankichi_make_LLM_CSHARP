# TinyLLM - C# ポート版

C++版のTinyLLMをC#で再実装した、教育用の小型言語モデルです。

## 特徴

- **可読性重視**: C#の構文で理解しやすい実装
- **CPU実装**: GPU/CUDAに依存しない
- **永続性**: モデルチェックポイントの保存・読み込み
- **テキストベース学習**: 外部ファイルから訓練データを読み込み
- **推論機能**: 次のトークン予測
- **日本語コメント**: 教育目的の詳細な解説

## プロジェクト構成

```
TinyLLMCSharp/
├── src/
│   ├── Tensor.cs              # テンソル（多次元配列）クラス
│   ├── Math.cs                # 数学演算（行列乗算、活性化関数など）
│   ├── SimpleTokenizer.cs      # テキスト → トークンID変換
│   ├── Transformer.cs          # Transformerレイヤー・モデル実装
│   └── Program.cs              # メインプログラム
├── data/
│   └── training_data.txt       # 訓練用テキストデータ
├── TinyLLMCSharp.csproj        # プロジェクト設定ファイル
└── README_JP.md                # このファイル
```

## クイックスタート

### 1. プロジェクトをビルド
```bash
dotnet build
```

### 2. 訓練と推論を実行
```bash
# 訓練 → 推論（デフォルト）
dotnet run

# 訓練のみ
dotnet run train

# 推論のみ
dotnet run infer

# 訓練 + 推論
dotnet run both
```

## ファイル説明

### Tensor.cs
多次元配列を管理するクラスです。ニューラルネットワークの重みと活性化値を保持します。

**主要メソッド:**
- `RandomInit()`: 正規分布 N(0, 0.01) でランダム初期化
- `Zero()`: すべての値をゼロにクリア
- `Get(row, col)`: 要素にアクセス
- `Set(row, col, value)`: 要素を設定

### Math.cs
ニューラルネットワークの基本演算を実装しています。

**実装関数:**
- `Matmul()`: 行列乗算（C = A @ B）
- `Relu()`: ReLU活性化関数
- `Softmax()`: ソフトマックス関数
- `CrossEntropyLoss()`: 損失関数（次のトークン予測の学習指標）

### SimpleTokenizer.cs
テキストをトークン（単語ID）に変換します。

**主要機能:**
- `BuildVocabulary()`: 訓練データから語彙を動的に構築
- `Tokenize()`: テキストをトークンID配列に変換
- `DeTokenize()`: トークンIDを単語に変換

### Transformer.cs
言語モデルの中核となるTransformerレイヤーとTinyLLMモデルを実装します。

**TransformerLayer:**
- マルチヘッド自己注意（簡易実装）
- フィードフォワードネットワーク（FFN）
- 残差接続

**TinyLLM:**
- Embedding層（トークン → ベクトル）
- Transformerレイヤー × 複数
- 出力層（隠れ状態 → 語彙確率分布）

**主要メソッド:**
- `Forward()`: トークンID → 予測確率分布
- `TrainStep()`: 1ステップの訓練（損失計算）
- `Predict()`: 次のトークン予測
- `SaveModel()`: モデルをファイルに保存
- `LoadModel()`: ファイルからモデルを読み込み

### Program.cs
メインプログラムです。訓練と推論を統合管理します。

**主要関数:**
- `Main()`: エントリーポイント、コマンドライン引数処理
- `LoadTrainingData()`: ファイルから訓練データを読み込み

**訓練設定:**
```
VOCAB_SIZE = 128        # 語彙サイズ
HIDDEN_DIM = 128        # 隠れ層の次元
NUM_LAYERS = 2          # Transformerレイヤー数
SEQ_LENGTH = 16         # シーケンス長
EPOCHS = 10             # 訓練エポック数
STEPS_PER_EPOCH = 3     # 1エポックあたりのステップ数
LEARNING_RATE = 0.001   # 学習率
```

## training_data.txt のカスタマイズ

`training_data.txt` を編集してモデルに異なるデータで学習させることができます。

**形式:**
- 1行1文（英文推奨）
- `#` で始まる行はコメント（スキップされます）
- 空行は無視されます

**例:**
```
# 動物に関するテキスト
The cat is sleeping
Dogs bark loudly
Birds sing in the morning

# 人間に関するテキスト（このセクションはコメント）
People like to walk
```

## LLMの学習原理

### 次のトークン予測（Next-Token Prediction）

TinyLLMは**次のトークン予測タスク**で学習します。

**例:**
- 入力: "I am a"
- 目標: "cat"
- モデルは入力から "cat" を予測することで学習

この単純なタスクが、言語の構造と意味を学習するのに効果的です。

### メモリと永続性

モデルは訓練後、`model_checkpoint.dat` というバイナリファイルに保存されます。

次に実行する際は、このファイルから読み込んで続きから訓練できます：

```
1回目の実行: 新規モデル作成 → 10エポック訓練 → checkpoint保存
2回目の実行: checkpointから読み込み → さらに10エポック訓練 → checkpoint上書き
```

## C++ 版との比較

| 項目 | C++ 版 | C# 版 |
|------|--------|--------|
| 可読性 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| GPU対応 | ⭐⭐⭐ | ⭐ |
| パフォーマンス | ⭐⭐⭐⭐ | ⭐⭐⭐ |
| デバッグの容易さ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| クロスプラットフォーム | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

## 実装の特徴と制限

### 簡易実装
- **マルチヘッド注意**: 簡易実装（真のマルチヘッド注意ではない）
- **勾配計算**: 簡略化（確率的勾配降下法による近似）
- **最適化**: 学習率固定（Adam等の適応的最適化なし）

### 学習効果
- データが限定的（15文）なため、実用的な言語理解能力は限定的
- 主に学習メカニズムの理解が目的
- より大規模なデータセットでの性能向上を期待

## 実行例

### 訓練フェーズの出力
```
[データ] 訓練データを読み込んでいます...
[データ] 15 件の訓練文を読み込みました: data/training_data.txt
[トークナイザー] 語彙を構築しています...
[Tokenizer] 語彙サイズ: 27 (25 個の一意の単語 + 特別トークン2個)

[訓練] エポック 1/10
  ステップ 1/3: Loss = 4.85203
  ステップ 2/3: Loss = 4.82105
  ステップ 3/3: Loss = 4.79876
...
[Model] チェックポイント保存: model_checkpoint.dat
```

### 推論フェーズの出力
```
[推論] 次のトークン予測:

  入力: "I am a"
  予測: "dog"

  入力: "The cat is"
  予測: "cute"

  入力: "I like"
  予測: "cats"

  入力: "Cats are"
  予測: "animals"
```

## 今後の拡張案

1. **より高度なアテンション機構**: 真のマルチヘッド注意実装
2. **適応的最適化**: Adam、RMSprop等の実装
3. **より大規模なモデル**: レイヤー数や隠れ次元の拡張
4. **異なる言語**: 日本語、中国語等への対応
5. **API化**: WebAPI として提供
6. **ビジュアライゼーション**: 注意度のビジュアル表示

## ライセンス

教育目的での使用を想定しています。

## 関連リンク

- [C++ 版 TinyLLM](https://github.com/takuya-sawa/Nyankichi_make_LLM)

---

作成日: 2025年12月13日
最終更新: 2025年12月13日
